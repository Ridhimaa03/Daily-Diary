{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwamFlSuYuEJ",
        "outputId": "4fdc6a07-67e2-4ca5-8e6e-420659a7b298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7  ...  0.09999999999999998\n",
            "Input: (0, 0)  Output: 1\n",
            "Input: (0, 1)  Output: 1\n",
            "Input: (1, 0)  Output: 1\n",
            "Input: (1, 1)  Output: 1\n"
          ]
        }
      ],
      "source": [
        "w1, w2 = 1.2, 0.6\n",
        "def activate(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def train_perceptron(inputs, desired_outputs, learning_rate, epochs):\n",
        "    global w1, w2\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            A, B = inputs[i]\n",
        "            target_output = desired_outputs[i]\n",
        "            output = activate(w1 * A + w2 * B)\n",
        "            error = target_output - output\n",
        "            w1 += learning_rate * error * A\n",
        "            w2 += learning_rate * error * B\n",
        "            total_error += error\n",
        "        if total_error == 0:\n",
        "            break\n",
        "    print(w1,\" ... \", w2)\n",
        "\n",
        "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "desired_outputs = [0, 0, 0, 1]\n",
        "learning_rate = 0.5\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "train_perceptron(inputs, desired_outputs, learning_rate, epochs)\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "    A, B = inputs[i]\n",
        "    output = activate(w1 * A + w2 * B)\n",
        "    print(f\"Input: ({A}, {B})  Output: {output}\")\n",
        "# print(w1,\" ... \", w2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = 0.6, 0.6\n",
        "def activate(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def train_perceptron(inputs, desired_outputs, learning_rate, epochs):\n",
        "    global w1, w2, b\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            A, B = inputs[i]\n",
        "            target_output = desired_outputs[i]\n",
        "            output = activate(w1 * A + w2 * B)\n",
        "            error = target_output - output\n",
        "            w1 += learning_rate * error * A\n",
        "            w2 += learning_rate * error * B\n",
        "\n",
        "            total_error += abs(error)\n",
        "        if total_error == 0:\n",
        "            break\n",
        "    print(w1,\" ... \", w2)\n",
        "\n",
        "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "desired_outputs = [0, 1, 1, 1]\n",
        "learning_rate = 0.5\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "train_perceptron(inputs, desired_outputs, learning_rate, epochs)\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "    A, B = inputs[i]\n",
        "    output = activate(w1 * A + w2 * B)\n",
        "    print(f\"Input: ({A}, {B})  Output: {output}\")\n",
        "# print(w1,\" ... \", w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmfASMmUb1HA",
        "outputId": "46ccd539-f846-44a7-83e5-d4e7d9da4bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6  ...  0.6\n",
            "Input: (0, 0)  Output: 1\n",
            "Input: (0, 1)  Output: 1\n",
            "Input: (1, 0)  Output: 1\n",
            "Input: (1, 1)  Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights for the hidden layer perceptrons\n",
        "w1_1, w1_2 = 1.0, 1.0  # Weights for the first hidden neuron\n",
        "w2_1, w2_2 = 1.0, 1.0  # Weights for the second hidden neuron\n",
        "\n",
        "# Weights for the output perceptron\n",
        "w3_1, w3_2 = 1.0, 1.0  # These combine the outputs of the two hidden neurons\n",
        "\n",
        "# Biases for the hidden layer and output layer\n",
        "b1, b2, b3 = 0,0,0  # biases for the two hidden neurons and the output neuron\n",
        "\n",
        "def activate(x):\n",
        "    return 1 if x >= 1 else 0\n",
        "\n",
        "def xor_perceptron(inputs, learning_rate, epochs):\n",
        "    global w1_1, w1_2, w2_1, w2_2, w3_1, w3_2, b1, b2, b3\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            A, B = inputs[i][0], inputs[i][1]\n",
        "            target_output = inputs[i][2]\n",
        "\n",
        "            # First hidden neuron: acts like an OR gate\n",
        "            weighted_sum_1 = w1_1 * A + w1_2 * B + b1\n",
        "            output_1 = activate(weighted_sum_1)\n",
        "\n",
        "            # Second hidden neuron: acts like a NAND gate\n",
        "            weighted_sum_2 = w2_1 * A + w2_2 * B + b2\n",
        "            output_2 = activate(weighted_sum_2)\n",
        "\n",
        "            # Output neuron: acts like an AND gate\n",
        "            weighted_sum_3 = w3_1 * output_1 + w3_2 * output_2 + b3\n",
        "            output_3 = activate(weighted_sum_3)\n",
        "\n",
        "            # Compute error\n",
        "            error = target_output - output_3\n",
        "\n",
        "            # Update weights for the hidden neurons and the output neuron\n",
        "            if error != 0:\n",
        "                w3_1 += learning_rate * error * output_1\n",
        "                w3_2 += learning_rate * error * output_2\n",
        "                b3 += learning_rate * error\n",
        "\n",
        "            total_error += abs(error)\n",
        "\n",
        "        if total_error == 0:\n",
        "            break\n",
        "\n",
        "    print(f\"Final weights for hidden layer: w1_1 = {w1_1}, w1_2 = {w1_2}, w2_1 = {w2_1}, w2_2 = {w2_2}\")\n",
        "    print(f\"Final weights for output layer: w3_1 = {w3_1}, w3_2 = {w3_2}\")\n",
        "    print(f\"Final biases: b1 = {b1}, b2 = {b2}, b3 = {b3}\")\n",
        "\n",
        "# XOR inputs and their expected outputs\n",
        "inputs = [\n",
        "    (0, 0, 0),\n",
        "    (0, 1, 1),\n",
        "    (1, 0, 1),\n",
        "    (1, 1, 0)\n",
        "]\n",
        "\n",
        "learning_rate = 1.5\n",
        "epochs = 10000\n",
        "\n",
        "# Train the XOR perceptron\n",
        "xor_perceptron(inputs, learning_rate, epochs)\n",
        "\n",
        "# Test the trained XOR perceptron\n",
        "print(\"\\nTesting XOR gate:\")\n",
        "for i in range(len(inputs)):\n",
        "    A, B = inputs[i][0], inputs[i][1]\n",
        "\n",
        "    # First hidden neuron: OR gate\n",
        "    weighted_sum_1 = w1_1 * A + w1_2 * B + b1\n",
        "    output_1 = activate(weighted_sum_1)\n",
        "\n",
        "    # Second hidden neuron: NAND gate\n",
        "    weighted_sum_2 = w2_1 * A + w2_2 * B + b2\n",
        "    output_2 = activate(weighted_sum_2)\n",
        "\n",
        "    # Output neuron: AND gate\n",
        "    weighted_sum_3 = w3_1 * output_1 + w3_2 * output_2 + b3\n",
        "    output_3 = activate(weighted_sum_3)\n",
        "\n",
        "    print(f\"Input: ({A}, {B})  Output: {output_3}\")\n"
      ],
      "metadata": {
        "id": "STd8WwJ4nfbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de3e996-cfd1-4549-fa39-aa0c32a595c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights for hidden layer: w1_1 = 1.0, w1_2 = 1.0, w2_1 = 1.0, w2_2 = 1.0\n",
            "Final weights for output layer: w3_1 = -0.5, w3_2 = -0.5\n",
            "Final biases: b1 = 0, b2 = 0, b3 = -1.5\n",
            "\n",
            "Testing XOR gate:\n",
            "Input: (0, 0)  Output: 0\n",
            "Input: (0, 1)  Output: 0\n",
            "Input: (1, 0)  Output: 0\n",
            "Input: (1, 1)  Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights for the hidden layer perceptrons\n",
        "w1_1, w1_2 = 0.5, 0.5  # Weights for the first hidden neuron (OR-like)\n",
        "w2_1, w2_2 = -1.0, -1.0  # Weights for the second hidden neuron (NAND-like)\n",
        "\n",
        "# Weights for the output perceptron\n",
        "w3_1, w3_2 = 1.0, 1.0  # These combine the outputs of the two hidden neurons\n",
        "\n",
        "# Biases for the hidden layer and output layer\n",
        "b1, b2, b3 = -0.5, 1.5, -1.0  # biases for the two hidden neurons and the output neuron\n",
        "\n",
        "def activate(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def xor_perceptron(inputs, learning_rate, epochs):\n",
        "    global w1_1, w1_2, w2_1, w2_2, w3_1, w3_2, b1, b2, b3\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            A, B = inputs[i][0], inputs[i][1]\n",
        "            target_output = inputs[i][2]\n",
        "\n",
        "            # First hidden neuron: OR-like\n",
        "            weighted_sum_1 = w1_1 * A + w1_2 * B + b1\n",
        "            output_1 = activate(weighted_sum_1)\n",
        "\n",
        "            # Second hidden neuron: NAND-like\n",
        "            weighted_sum_2 = w2_1 * A + w2_2 * B + b2\n",
        "            output_2 = activate(weighted_sum_2)\n",
        "\n",
        "            # Output neuron: AND-like combining the two hidden neurons\n",
        "            weighted_sum_3 = w3_1 * output_1 + w3_2 * output_2 + b3\n",
        "            output_3 = activate(weighted_sum_3)\n",
        "\n",
        "            # Compute error\n",
        "            error = target_output - output_3\n",
        "\n",
        "            # Update weights for the output neuron\n",
        "            w3_1 += learning_rate * error * output_1\n",
        "            w3_2 += learning_rate * error * output_2\n",
        "            b3 += learning_rate * error\n",
        "\n",
        "            # Update weights for the hidden neurons (only if there is an error)\n",
        "            if error != 0:\n",
        "                w1_1 += learning_rate * error * A\n",
        "                w1_2 += learning_rate * error * B\n",
        "                w2_1 += learning_rate * error * A\n",
        "                w2_2 += learning_rate * error * B\n",
        "\n",
        "            total_error += abs(error)\n",
        "\n",
        "        if total_error == 0:\n",
        "            break\n",
        "\n",
        "    print(f\"Final weights for hidden layer: w1_1 = {w1_1}, w1_2 = {w1_2}, w2_1 = {w2_1}, w2_2 = {w2_2}\")\n",
        "    print(f\"Final weights for output layer: w3_1 = {w3_1}, w3_2 = {w3_2}\")\n",
        "    print(f\"Final biases: b1 = {b1}, b2 = {b2}, b3 = {b3}\")\n",
        "\n",
        "# XOR inputs and their expected outputs\n",
        "inputs = [\n",
        "    (0, 0, 0),\n",
        "    (0, 1, 1),\n",
        "    (1, 0, 1),\n",
        "    (1, 1, 0)\n",
        "]\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Train the XOR perceptron\n",
        "xor_perceptron(inputs, learning_rate, epochs)\n",
        "\n",
        "# Test the trained XOR perceptron\n",
        "print(\"\\nTesting XOR gate:\")\n",
        "for i in range(len(inputs)):\n",
        "    A, B = inputs[i][0], inputs[i][1]\n",
        "\n",
        "    # First hidden neuron: OR gate\n",
        "    weighted_sum_1 = w1_1 * A + w1_2 * B + b1\n",
        "    output_1 = activate(weighted_sum_1)\n",
        "\n",
        "    # Second hidden neuron: NAND gate\n",
        "    weighted_sum_2 = w2_1 * A + w2_2 * B + b2\n",
        "    output_2 = activate(weighted_sum_2)\n",
        "\n",
        "    # Output neuron: AND gate\n",
        "    weighted_sum_3 = w3_1 * output_1 + w3_2 * output_2 + b3\n",
        "    output_3 = activate(weighted_sum_3)\n",
        "\n",
        "    print(f\"Input: ({A}, {B})  Output: {output_3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJlwnd_l7TUL",
        "outputId": "006fa07a-cd8a-4f56-e24a-0371b650793e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights for hidden layer: w1_1 = 0.5, w1_2 = 0.5, w2_1 = -1.0, w2_2 = -1.0\n",
            "Final weights for output layer: w3_1 = 1.0, w3_2 = 0.9\n",
            "Final biases: b1 = -0.5, b2 = 1.5, b3 = -1.1\n",
            "\n",
            "Testing XOR gate:\n",
            "Input: (0, 0)  Output: 0\n",
            "Input: (0, 1)  Output: 1\n",
            "Input: (1, 0)  Output: 1\n",
            "Input: (1, 1)  Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRqMRG6q_xI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3-YX2WHmbsR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}